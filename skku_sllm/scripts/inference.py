#!/usr/bin/env python
# -*- coding: utf-8 -*-

import argparse
import torch
import time
import os
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import PeftModel

# GPU ì„¤ì • - ëª¨ë“  ê°€ìš© GPU ì‚¬ìš©
os.environ["CUDA_VISIBLE_DEVICES"] = "0,1"  # ë‘ ê°œì˜ GPU ëª¨ë‘ ì‚¬ìš©

def parse_args():
    parser = argparse.ArgumentParser(description='ì„±ê· ê´€ëŒ€í•™êµ LLM ì±„íŒ…')
    
    parser.add_argument(
        '--model_path', 
        type=str, 
        default='/home/work/.deep_learning/skkullm/skku_sllm/models/finetuned/skku-llm-20250511/checkpoint-351',
        help='íŒŒì¸íŠœë‹ëœ ëª¨ë¸ ê²½ë¡œ (ì—†ìœ¼ë©´ ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš©)'
    )
    
    parser.add_argument(
        '--base_model_path', 
        type=str, 
        default="/home/work/.deep_learning/skkullm/skku_sllm/models/base/Llama-3-Open-Ko-8B",
        help='ê¸°ë³¸ ëª¨ë¸ ê²½ë¡œ'
    )
    
    parser.add_argument(
        '--max_tokens', 
        type=int, 
        default=256,
        help='ìµœëŒ€ ìƒì„± í† í° ìˆ˜'
    )
    
    return parser.parse_args()

def format_prompt(instruction, input_text=None):
    """ì‚¬ìš©ì ì…ë ¥ì„ ëª¨ë¸ ì…ë ¥ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    if input_text:
        return f"### ì§€ì‹œë¬¸:\n{instruction}\n\n### ì…ë ¥:\n{input_text}\n\n### ì‘ë‹µ:\n"
    return f"### ì§€ì‹œë¬¸:\n{instruction}\n\n### ì‘ë‹µ:\n"

def get_optimal_device_map():
    """
    ì‚¬ìš© ê°€ëŠ¥í•œ GPUë¥¼ í™•ì¸í•˜ê³  ìµœì ì˜ device_map ë°˜í™˜
    """
    num_gpus = torch.cuda.device_count()
    print(f"ì‚¬ìš© ê°€ëŠ¥í•œ GPU ìˆ˜: {num_gpus}")
    
    if num_gpus == 0:
        return "cpu"
    elif num_gpus == 1:
        return "auto"
    else:
        # ì—¬ëŸ¬ ê°œì˜ GPUì— ëª¨ë¸ì„ ë¶„ì‚° (ì¸µ ê¸°ë°˜ ë³‘ë ¬í™”)
        return "balanced"

def load_model(args):
    """ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ"""
    print(f"ëª¨ë¸ ë¡œë“œ ì¤‘... ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”...")
    
    # ìµœì  ì¥ì¹˜ ë§µ ê²°ì •
    device_map = get_optimal_device_map()
    print(f"ì‚¬ìš©í•  ì¥ì¹˜ ë§µ: {device_map}")
    
    # GPU ë©”ëª¨ë¦¬ ì •ë³´ ì¶œë ¥
    if torch.cuda.is_available():
        for i in range(torch.cuda.device_count()):
            free_mem = torch.cuda.get_device_properties(i).total_memory / (1024**3)
            print(f"GPU {i}: {torch.cuda.get_device_name(i)}, ë©”ëª¨ë¦¬: {free_mem:.2f} GB")
    
    # ì–‘ìí™” ì„¤ì • (ì¶”ë¡  ì†ë„ ê°œì„ )
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.float16
    )
    
    # ê¸°ë³¸ ëª¨ë¸ ë¡œë“œ
    base_model = AutoModelForCausalLM.from_pretrained(
        args.base_model_path,
        quantization_config=bnb_config,
        device_map=device_map,  # ë©€í‹° GPUì— ë°¸ëŸ°ìŠ¤ ìˆê²Œ ë¶„ì‚°
        trust_remote_code=True,
        local_files_only=True  # ë¡œì»¬ íŒŒì¼ë§Œ ì‚¬ìš©
    )
    
    # í† í¬ë‚˜ì´ì € ë¡œë“œ
    tokenizer = AutoTokenizer.from_pretrained(
        args.base_model_path,
        trust_remote_code=True,
        local_files_only=True  # ë¡œì»¬ íŒŒì¼ë§Œ ì‚¬ìš©
    )
    
    # íŒŒì¸íŠœë‹ëœ ëª¨ë¸ì´ ìˆìœ¼ë©´ ë¡œë“œ
    if args.model_path:
        print(f"íŒŒì¸íŠœë‹ëœ ëª¨ë¸ ë¡œë“œ ì¤‘: {args.model_path}")
        model = PeftModel.from_pretrained(base_model, args.model_path)
    else:
        model = base_model
    
    return model, tokenizer

def generate_response(model, tokenizer, prompt, max_new_tokens=32):
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    start_time = time.time()
    with torch.no_grad():
        output = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            do_sample=True,
            temperature=0.8,
            top_p=0.7,
            top_k=3,
            num_beams=8,
            early_stopping=True,
            repetition_penalty=1.2,
            no_repeat_ngram_size=3,
            eos_token_id=tokenizer.eos_token_id,  # â† ë°˜ë“œì‹œ ì¶”ê°€!
        )
    response = tokenizer.decode(output[0], skip_special_tokens=True)
    response = response.replace(prompt, "").strip()
    end_time = time.time()
    print(f"[ì‘ë‹µ ìƒì„± ì‹œê°„: {end_time - start_time:.2f}ì´ˆ]")
    return response

def print_welcome():
    """í™˜ì˜ ë©”ì‹œì§€ ì¶œë ¥"""
    print("\n" + "="*50)
    print("ğŸ« ì„±ê· ê´€ëŒ€í•™êµ íŠ¹í™” LLM ì±„íŒ…ì„ ì‹œì‘í•©ë‹ˆë‹¤ ğŸ«")
    print("="*50)
    print("- ì„±ê· ê´€ëŒ€í•™êµ ì •ë³´ì™€ ê´€ë ¨ëœ ì§ˆë¬¸ì„ í•´ë³´ì„¸ìš”!")
    print("- ì¢…ë£Œí•˜ë ¤ë©´ 'exit' ë˜ëŠ” 'ì¢…ë£Œ'ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
    print("="*50 + "\n")

def main():
    args = parse_args()
    
    # ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
    model, tokenizer = load_model(args)
    
    # ì„±ëŠ¥ ìµœì í™”ë¥¼ ìœ„í•œ ì„¤ì •
    model.config.use_cache = True  # KV ìºì‹œ í™œì„±í™”
    
    # í™˜ì˜ ë©”ì‹œì§€ ì¶œë ¥
    print_welcome()
    
    # ì±„íŒ… ë£¨í”„
    while True:
        # ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°
        user_input = input("ğŸ‘¤ ì§ˆë¬¸: ")
        
        # ì¢…ë£Œ ì¡°ê±´ í™•ì¸
        if user_input.lower() in ["exit", "quit", "ì¢…ë£Œ", "bye"]:
            print("\nğŸ« ëŒ€í™”ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤. ì„±ê· ê´€ëŒ€ì™€ í•¨ê»˜í•´ ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤! ğŸ‘‹")
            break
        
        # ë¹ˆ ì…ë ¥ ì²˜ë¦¬
        if not user_input.strip():
            continue
        
        # ì…ë ¥ì´ ìˆëŠ”ì§€ í™•ì¸ (ì˜ˆ: "ì§ˆë¬¸: ì…ë ¥" í˜•ì‹)
        if ":" in user_input:
            instruction, input_text = user_input.split(":", 1)
            instruction = instruction.strip()
            input_text = input_text.strip()
        else:
            instruction = user_input
            input_text = None
        
        # ëª¨ë¸ ì‘ë‹µ ìƒì„±
        prompt = format_prompt(instruction, input_text)
        response = generate_response(model, tokenizer, prompt, args.max_tokens)
        
        # ëª¨ë¸ ì‘ë‹µ ì¶œë ¥
        print(f"ğŸ¤– ì‘ë‹µ: {response}\n")

if __name__ == "__main__":
    main() 