import torch
from transformers import AutoModelForCausalLM
from peft import PeftModel, PeftConfig

# === κ²½λ΅ μ„¤μ • ===
BASE_MODEL_PATH = "/home/work/.deep_learning/skkullm/skku_sllm/models/base/Llama-3-Open-Ko-8B"
ADAPTER_PATH = "/home/work/.deep_learning/skkullm/skku_sllm/models/finetuned/skku-llm-20250505/checkpoint-4008_adapter_only"

# === λ¨λΈ λ΅λ“ ===
print("β… Base λ¨λΈ λ΅λ“ μ¤‘...")
base = AutoModelForCausalLM.from_pretrained(BASE_MODEL_PATH)
print("β… LoRA adapter μ μ© λ¨λΈ λ΅λ“ μ¤‘...")
model = PeftModel.from_pretrained(base, ADAPTER_PATH)

# === νλΌλ―Έν„° μ κ³„μ‚° ===
total_params = 0
lora_params = 0

for name, param in model.named_parameters():
    total_params += param.numel()
    if "lora_" in name:  # LoRA κ΄€λ ¨ νλΌλ―Έν„°λ§ ν•©μ‚°
        lora_params += param.numel()

# === κ²°κ³Ό μ¶λ ¥ ===
print("\nπ“ LoRA νλΌλ―Έν„° λ¶„μ„ κ²°κ³Ό")
print(f" - μ „μ²΄ νλΌλ―Έν„° μ       : {total_params:,}")
print(f" - LoRA ν•™μµ νλΌλ―Έν„° μ  : {lora_params:,}")
print(f" - LoRA κ²½λ‰ν™” λΉ„μ¨       : {100 * lora_params / total_params:.4f}%")
